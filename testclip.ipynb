{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in ./.venv/lib/python3.10/site-packages (10.2.0)\r\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.2.1)\r\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.10/site-packages (0.17.1)\r\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (4.66.2)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.10/site-packages (from torch) (4.10.0)\r\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch) (2024.3.1)\r\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from torchvision) (1.26.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install pillow torch torchvision tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T18:41:19.997552Z",
     "start_time": "2024-03-23T18:41:19.317828Z"
    }
   },
   "id": "cbd6d0d4398beae2",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from clip_original import load\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T18:41:21.046527Z",
     "start_time": "2024-03-23T18:41:20.010691Z"
    }
   },
   "id": "49303741bfbe5c2b",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = load(\"ViT-B/32\")\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T18:41:23.001493Z",
     "start_time": "2024-03-23T18:41:21.047389Z"
    }
   },
   "id": "1ce49024fd6f349d",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/modestyachts/ImageNetV2_pytorch\r\n",
      "  Cloning https://github.com/modestyachts/ImageNetV2_pytorch to /private/var/folders/w6/q5k5xxgn6xg0tsdx66llblg40000gn/T/pip-req-build-h9e13eu2\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/modestyachts/ImageNetV2_pytorch /private/var/folders/w6/q5k5xxgn6xg0tsdx66llblg40000gn/T/pip-req-build-h9e13eu2\r\n",
      "  Resolved https://github.com/modestyachts/ImageNetV2_pytorch to commit 14d4456c39fe7f02a665544dd9fc37c1a5f8b635\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h"
     ]
    }
   ],
   "source": [
    "! pip install git+https://github.com/modestyachts/ImageNetV2_pytorch\n",
    "\n",
    "from imagenetv2_pytorch import ImageNetV2Dataset\n",
    "\n",
    "images = ImageNetV2Dataset(transform=preprocess)\n",
    "loader = torch.utils.data.DataLoader(images, batch_size=32, num_workers=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T18:41:25.382656Z",
     "start_time": "2024-03-23T18:41:23.002643Z"
    }
   },
   "id": "96c6e8c26dc4d814",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/313 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "afb99a591780499297afd1a1385c35c2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1377,  0.2515,  0.2710,  ...,  0.7292, -0.2895,  0.3732],\n",
      "        [-0.0074,  0.1267, -0.1033,  ...,  0.8017, -0.2914,  0.1355],\n",
      "        [ 0.0645,  0.2135, -0.0076,  ...,  0.7485, -0.1318,  0.0283],\n",
      "        ...,\n",
      "        [-0.0575, -0.2602, -0.0947,  ..., -0.3892,  0.0955, -0.3477],\n",
      "        [ 0.0337, -0.4330, -0.3819,  ...,  0.5730, -0.0393,  0.0719],\n",
      "        [ 0.1686, -0.2909,  0.3366,  ...,  0.0359,  0.3826, -0.2745]])\n",
      "torch.Size([32, 512])\n",
      "tensor([[-1.1417e-01, -7.6141e-02,  2.2491e-01,  ...,  5.3485e-01,\n",
      "          4.7066e-01, -2.2261e-01],\n",
      "        [-1.6877e-01, -3.2321e-01, -2.6901e-02,  ...,  2.6000e-03,\n",
      "          5.0066e-01, -5.4945e-04],\n",
      "        [ 1.7300e-01,  1.6507e-03,  1.8470e-02,  ...,  4.9814e-01,\n",
      "          3.5767e-01, -2.5597e-01],\n",
      "        ...,\n",
      "        [ 2.1425e-02, -3.2792e-01, -2.7887e-01,  ...,  7.2249e-01,\n",
      "         -9.4490e-03, -1.0335e-01],\n",
      "        [ 3.8274e-01, -1.9670e-01, -2.5411e-01,  ...,  2.8039e-01,\n",
      "          2.1960e-01,  1.8083e-01],\n",
      "        [ 2.2058e-01,  6.2799e-02, -2.3926e-01,  ...,  6.9927e-01,\n",
      "          1.3546e-01, -1.0644e-01]])\n",
      "torch.Size([32, 512])\n",
      "tensor([[-0.0051,  0.0119, -0.1623,  ...,  0.7323, -0.2331, -0.2417],\n",
      "        [ 0.1670, -0.1772, -0.4338,  ...,  0.7385, -0.0500,  0.1140],\n",
      "        [ 0.0595, -0.0608, -0.2956,  ...,  0.7745, -0.1721,  0.0379],\n",
      "        ...,\n",
      "        [ 0.0410,  0.4304, -0.1124,  ...,  0.5689, -0.0901, -0.2388],\n",
      "        [ 0.0298,  0.5380, -0.0131,  ...,  0.2710, -0.0425,  0.3457],\n",
      "        [-0.1985,  0.6741, -0.2046,  ...,  0.6024,  0.2627, -0.0037]])\n",
      "torch.Size([32, 512])\n",
      "tensor([[-0.2019,  0.1760,  0.0101,  ...,  0.4213,  0.2079, -0.1723],\n",
      "        [-0.0464,  0.5085,  0.1154,  ...,  0.3444,  0.0592, -0.4976],\n",
      "        [ 0.1638,  0.5128,  0.2969,  ...,  0.6285,  0.5610,  0.1043],\n",
      "        ...,\n",
      "        [-0.0992, -0.0241, -0.1190,  ...,  0.4169, -0.0440, -0.2805],\n",
      "        [-0.2803, -0.2813, -0.2255,  ...,  0.2928,  0.1542, -0.0275],\n",
      "        [ 0.1514, -0.3350, -0.1795,  ...,  0.4513, -0.0268, -0.1204]])\n",
      "torch.Size([32, 512])\n",
      "tensor([[ 0.1054, -0.1550, -0.2503,  ...,  0.4951,  0.2183,  0.2664],\n",
      "        [-0.1042,  0.0553, -0.0399,  ...,  0.6151,  0.2505, -0.2635],\n",
      "        [ 0.3327, -0.0629,  0.0693,  ...,  0.0514,  0.0173, -0.4733],\n",
      "        ...,\n",
      "        [-0.1531,  0.0137,  0.1018,  ...,  0.9626, -0.2624,  0.0623],\n",
      "        [-0.0245,  0.4386, -0.1609,  ...,  0.1785, -0.4257, -0.1513],\n",
      "        [-0.1166, -0.0464, -0.2084,  ...,  0.4283,  0.2764, -0.2020]])\n",
      "torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    top1, top5, n = 0., 0., 0.\n",
    "    for i, (images, target) in enumerate(tqdm(loader)):\n",
    "        # images = images\n",
    "        # target = target\n",
    "        \n",
    "        # predict\n",
    "        image_features = model.encode_image(images)\n",
    "        if i > 4:\n",
    "            break\n",
    "        \n",
    "        print(image_features)\n",
    "        print(image_features.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T18:41:40.042390Z",
     "start_time": "2024-03-23T18:41:25.384115Z"
    }
   },
   "id": "c28885d525014091",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T18:41:40.046236Z",
     "start_time": "2024-03-23T18:41:40.043725Z"
    }
   },
   "id": "c04dc706396a5a3d",
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

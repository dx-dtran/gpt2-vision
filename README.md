# GPT-2 Vision

Learn how to build a vision-language model 

GPT-2 Vision is a language model that has been augmented with CLIP to understand images. Given an image, it is able to generate text describing that image

The model is an experiment in reproducing the training techniques rumored to be behind state-of-the-art multimodal models like GPT-4 Vision, but using much smaller models: GPT-2 small (124M) and CLIP ViT-B/32

The code is designed to be educational and is written from scratch in just 5 files, totaling under 1000 lines of pure PyTorch

<div align="center">
 <img src="assets/ski.png" width="700" />
 <br>
 <em>Caption generated by GPT-2 Vision on an unseen image of me</em>
</div>

## Overview

GPT-2 Vision works by feeding an image into CLIP, which generates visual embeddings that represent the meaning of the image in a language space. A multi-layer perceptron aligns these visual embeddings with GPT-2's specific language embedding space. GPT-2 uses these refined embeddings to produce a text description of the image

CLIP and GPT-2 are initialized with OpenAI's pre-trained model weights. The multi-layer perceptron is trained from randomly initialized parameters using image-text pairs from the [COCO dataset](https://cocodataset.org/#home)

## Background

### Language models

Given a sequence of words (a prompt), language models like GPT-2 predict the next likely word

For example:

A cat sat on a red ___.

A language model will likely predict "mat" instead of "lava" because the data it was trained on likely contains many examples of cats sitting on mats but few of cats sitting on lava

<div align="center">
 <img src="assets/gpt-architecture.png" width="500" />
</div>

*A GPT-2 language model converting a sequence of text into word embeddings, then using these embeddings together to predict the next word*

###  Text embeddings

To make these predictions, language models first convert the prompt into text or "word" embeddings. These embeddings can be thought of as points in a high-dimensional space where similar words are near one another

Regions in this space represent semantic meaning -- word embeddings for animals might reside in a particular space, whereas embeddings for sporting equipment might reside in another

### Predicting the next word using embeddings

To predict the next word, the model might naively find an embedding closest to the embeddings of all the words in the prompt. For example, given the sentence "The sun rises every ___," it might easily select "morning" because "sun" and "rises" are unambiguously associated with "morning" and would likely reside in a similar space

However, this approach could fail for words with multiple meanings like "bat" 

Consider the following sentences:

1. The bat flew out of the ___.
2. The player swung the bat at the ___.
 
Would the embedding space near "bat" represent the animal or sports equipment?

### Context-aware predictions

To solve this, language models use the full context of the prompt to effectively combine the embeddings, capturing the complete meaning of the text. The next word is then sampled from a region in the space close to this context-aware representation

In the first sentence, a language model might focus on "flew out," adjusting the embeddings to create a representation meaning "bat" as a flying animal. This refined embedding might be close to "cave" in the semantic space, so the model may predict "cave"

In the second sentence, a model might focus on "player swung," adjusting the embeddings to represent "bat" as sports equipment, leading the model to predict "baseball"

In sum, **language models transform word embeddings in a way that captures the full meaning of the text, making it easier to accurately find the next word.**

### CLIP

OpenAI CLIP is a model that finds similarities between images and text

CLIP consists of a vision encoder and a language encoder. The vision encoder generates visual embeddings from images, while the language encoder produces text embeddings, both in a shared high-dimensional semantic space

CLIP is trained to align these embeddings, assigning high similarity scores to matching image-text pairs and low scores to non-matching pairs. Once trained, CLIP ensures that the embedding of an image, like a cat, is close to text embeddings of related phrases in the semantic space

For example, flying bats are near animals, and baseball bats are near sporting equipment

### Vision-Language Models

A vision-language model generates text from images or text inputs

GPT-2, a pure-text language model, operates in its own text embedding space. To convert GPT-2 into a vision-language model, we need to map CLIP visual embeddings into GPT-2's language embedding space

This is achieved using a multi-layer perceptron (MLP), which transforms CLIP embeddings into the GPT-2 space. In the CLIP space, images of cats are near phrases about cats. Similarly, we want these visual embeddings to be near cat-related phrases in the GPT-2 space

An image embedding of the animal bat will be in a certain location in the space. GPT-2 can then use this context to generate a caption about animals and caves. An image embedding of a baseball bat will be in a different location, allowing GPT-2 to write about players, fields, and stadiums

When the CLIP visual embeddings and GPT-2 text embeddings are aligned, GPT-2 can generate text relevant to the image

<div align="center">
 <img src="assets/gpt-clip-architecture.png" width="800" />
</div>

*CLIP converts an image into a sequence of embeddings, which an MLP transforms into GPT-2's language embedding space. GPT-2 uses these image embeddings, along with any initial text, as context to generate text relevant to the image*

## Samples

| ![Image 1](assets/desert.png) | ![Image 2](assets/computer.png) |
|-------------------------------|---------------------------------|
| ![Image 3](assets/pond.png)   | ![Image 4](assets/food.png)     |
| ![Image 5](assets/zoo.png)    | ![Image 6](assets/mall.png)     |
| ![Image 7](assets/car.png)    | ![Image 8](assets/stadium.png)  |

*Captions generated by GPT-2 Vision on unseen images*

## Install

```shell
$ python -m venv venv
$ source venv/bin/activate
$ pip install -r requirements.txt
```

## Train

Download the GPT-2 and CLIP pre-trained model weights

The training code updates a multi-layer perceptron to align visual embeddings from CLIP with word embeddings from GPT-2

Train using an NVIDIA GPU with at least 6GB of VRAM

```shell
$ python train.py
```

## Run

Generate captions on your own images
```shell
$ python generate.py
```

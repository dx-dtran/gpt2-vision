# GPT-2 Vision

Learn how to build a vision-language model 

GPT-2 Vision is a language model that has been augmented with CLIP to understand images. Given an image, it is able to generate text describing that image

The model is an experiment in reproducing the training techniques rumored to be behind state-of-the-art multimodal models like GPT-4 Vision, but using much smaller models: GPT-2 small (124M) and CLIP ViT-B/32

The code is designed to be educational and is written from scratch in just 5 files, totaling under 1000 lines of pure PyTorch

<div align="center">
 <img src="assets/ski.png" width="700" />
 <br>
 <em>Caption generated by GPT-2 Vision on an unseen image of me</em>
</div>

## Overview

GPT-2 Vision works by feeding an image into CLIP, which generates visual embeddings that represent the meaning of the image in a language space. A multi-layer perceptron aligns these visual embeddings with GPT-2's specific language embedding space. GPT-2 uses these refined vision-language embeddings to produce a text description of the image

CLIP and GPT-2 are initialized with OpenAI's pre-trained model weights. The multi-layer perceptron is trained from randomly initialized weights using image-text pairs from the [COCO dataset](https://cocodataset.org/#home)

## Background

### Language models

Given a sequence of words (a prompt), language models like GPT-2 predict the next likely word

For example:

A cat sat on a red ___.

A language model will likely predict "mat" instead of "lava" because the data it was trained on likely contains many examples of cats sitting on mats but few of cats sitting on lava

<div align="center">
 <img src="assets/gpt-architecture.png" width="500" />
</div>

*A GPT-2 language model converts a sequence of text into word embeddings, then uses these embeddings together to predict the next word*

###  Text embeddings

To make these predictions, language models first convert the prompt into text or "word" embeddings. These embeddings can be thought of as points in a high-dimensional space where similar words are near one another

Regions in this space represent semantic meaning -- word embeddings for animals might reside in a particular space, whereas embeddings for sporting equipment might reside in another

### Predicting the next word using embeddings

To predict the next word, a model could naively find an embedding closest to the embeddings of all the words in the prompt. For example, given the sentence "The sun rises every ___," the model might easily select "morning" because "sun" and "rises" are unambiguously associated with "morning" and would likely reside in a similar space

However, this approach could fail for words with multiple meanings like "bat" 

Consider the following sentences:

1. The bat flew out of the ___.
2. The player swung the bat at the ___.
 
Would the embedding space near "bat" represent the animal or baseball equipment?

### Context-aware predictions

To solve this, language models use the full context of the prompt to effectively combine the embeddings, capturing the complete meaning of the text. The next word is then sampled from a region in the space close to this context-aware representation

In the first sentence, a language model might focus on "flew out," adjusting the embeddings to create a representation meaning "bat" as a flying animal. This refined embedding might be close to "cave" in the semantic space, so the model may predict "cave"

In the second sentence, a model might focus on "player swung," adjusting the embeddings to represent "bat" as sports equipment, leading the model to predict "baseball"

In sum, **language models transform word embeddings in a way that captures the full meaning of the text, making it easier to accurately find the next word**

### Visual embeddings with CLIP

Just as language embeddings represent text, visual embeddings represent images. OpenAI's CLIP is a model that builds visual embeddings by finding relationships between images and text

CLIP consists of a vision encoder and a language encoder. The vision encoder produces visual embeddings from images, while the language encoder produces text embeddings, both in a shared high-dimensional semantic space

Once trained, CLIP ensures that the embedding of an image, like a cat, is close to text embeddings of related phrases in the semantic space. For example, a properly trained CLIP model would place image embeddings of flying bats near both visual and language representations of animals, and embeddings of baseball bats near sporting equipment

### Vision-Language Models

A vision-language model generates text from images or text inputs

GPT-2, a pure-text language model, operates in its own unique text embedding space. To convert GPT-2 into a vision-language model, we use a multi-layer perceptron (MLP) to map CLIP visual embeddings into GPT-2's language embedding space

In the CLIP space, images of cats are near phrases about cats. After the MLP transformation, these visual embeddings will ideally remain near cat-related text embeddings, but now in the GPT-2 specific language space

Just as GPT-2 uses previous text embeddings to generate the next word, an aligned CLIP model uses visual embeddings as if they were text embeddings.

A pure-text language model uses the full context of the input text to generate relevant continuations. Similarly, a vision-language model uses CLIP visual embeddings to generate contextually appropriate descriptions. Once aligned, GPT-2 can use the full context of the image and text to generate coherent descriptions

<div align="center">
 <img src="assets/gpt-clip-architecture.png" width="800" />
</div>

*CLIP converts an image into a sequence of embeddings, which an MLP transforms into GPT-2's language embedding space. GPT-2 uses these image embeddings, along with any initial text, as context to generate text relevant to the image*

## Samples

| ![Image 1](assets/desert.png)   | ![Image 2](assets/computer.png)   |
|---------------------------------|-----------------------------------|
| ![Image 3](assets/pond.png)     | ![Image 4](assets/food.png)       |
| ![Image 5](assets/zoo.png)      | ![Image 6](assets/mall.png)       |
| ![Image 7](assets/car.png)      | ![Image 8](assets/stadium.png)    |

*Captions generated by GPT-2 Vision on unseen images*

## Install

```shell
$ python -m venv venv
$ source venv/bin/activate
$ pip install -r requirements.txt
```

## Train

Download the GPT-2 and CLIP pre-trained model weights

The training code updates a multi-layer perceptron to align visual embeddings from CLIP with word embeddings from GPT-2

Train using an NVIDIA GPU with at least 6GB of VRAM

```shell
$ python train.py
```

## Run

Generate captions on your own images
```shell
$ python generate.py
```
